{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base amazon url: https://www.amazon.de/SHASHIBO-Formwechsel-Zauberw%C3%BCrfel-Preisgekr%C3%B6nt-Seltenerdmagnete/dp/B07W5QM4DP/ref=cm_cr_arp_d_product_top?ie=UTF8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class Review:\n",
    "    title: str\n",
    "    text: str\n",
    "    rating: int\n",
    "    date: datetime\n",
    "\n",
    "    def to_file(self, path: Path):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(str(asdict(self)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import re\n",
    "\n",
    "def get_soup(content: str) -> BeautifulSoup:\n",
    "    \"\"\"Return a BeautifulSoup object from the given response\"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def has_customer_review_id(tag):\n",
    "    return tag.has_attr('id') and tag['id'].startswith('customer_review')\n",
    "\n",
    "def extract_date_str(date_element_text: str) -> str:\n",
    "    date_pattern = r'\\d+\\.\\s\\w+\\s\\d+'\n",
    "    date_match = re.search(date_pattern, date_element_text)\n",
    "    if date_match:\n",
    "        extracted_date = date_match.group()\n",
    "        return extracted_date\n",
    "    else:\n",
    "        return \"No date found\"\n",
    "\n",
    "\n",
    "def parse_to_date_object(date_str: str) -> datetime:\n",
    "    locale.setlocale(locale.LC_TIME, 'de_DE')\n",
    "    date_obj = datetime.strptime(date_str, \"%d. %B %Y\")\n",
    "    return date_obj\n",
    "\n",
    "def scrape_page(soup: BeautifulSoup, counter:int) -> int:\n",
    "    reviews_divs = soup.find_all(has_customer_review_id)\n",
    "    for review_div in reviews_divs:\n",
    "        print(f\"processing review {counter}\")\n",
    "        # Extract title\n",
    "        title = review_div.find('a', {'data-hook': 'review-title'}).text.strip()\n",
    "        \n",
    "        # Extract number of stars\n",
    "        stars = review_div.find('i', {'data-hook': 'review-star-rating'}).find('span').text.strip()\n",
    "        \n",
    "        # Extract review text - assuming it is contained in a p element with 'data-hook': 'review-body'\n",
    "        review_text = review_div.find('span', {'data-hook': 'review-body'}).text.strip()\n",
    "\n",
    "\n",
    "        # Extract date\n",
    "        date_element_text = review_div.find('span', {'data-hook': 'review-date'}).text.strip()\n",
    "        date_str = extract_date_str(date_element_text)\n",
    "        date_obj = parse_to_date_object(date_str)\n",
    "        \n",
    "\n",
    "        review = Review(title=title, text=review_text, rating=stars, date=date_obj)\n",
    "        review.to_file(Path(f'raw_reviews/review_{counter}.txt'))\n",
    "        counter += 1\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing review 0\n",
      "processing review 1\n",
      "processing review 2\n",
      "processing review 3\n",
      "processing review 4\n",
      "processing review 5\n",
      "processing review 6\n",
      "processing review 7\n",
      "processing review 8\n",
      "processing review 9\n",
      "Processed raw_pages\\page1.html with 10 reviews\n",
      "processing review 10\n",
      "processing review 11\n",
      "processing review 12\n",
      "processing review 13\n",
      "processing review 14\n",
      "processing review 15\n",
      "processing review 16\n",
      "processing review 17\n",
      "processing review 18\n",
      "processing review 19\n",
      "Processed raw_pages\\page2.html with 20 reviews\n",
      "processing review 20\n",
      "processing review 21\n",
      "processing review 22\n",
      "processing review 23\n",
      "processing review 24\n",
      "processing review 25\n",
      "processing review 26\n",
      "processing review 27\n",
      "processing review 28\n",
      "processing review 29\n",
      "Processed raw_pages\\page3.html with 30 reviews\n"
     ]
    }
   ],
   "source": [
    "pages = list(Path('raw_pages/').glob('*.html'))\n",
    "\n",
    "counter = 0\n",
    "for page in pages:\n",
    "    with open(page, 'r') as f:\n",
    "        content = f.read()\n",
    "        soup = get_soup(content)\n",
    "        counter = scrape_page(soup, counter)\n",
    "        print(f\"Processed {page} with {counter} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
